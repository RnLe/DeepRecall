{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca94d2e",
   "metadata": {},
   "source": [
    "# Step 1: Convert a Book to Markdown\n",
    "In this notebook, a book is converted to markdown format. The book can be in either PDF or EPUB format. The conversion is done using the custom `textProcessing` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d606d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found input file: Almanack/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_EBOOK_v103.epub\n",
      "Markdown version saved to: Almanack/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_EBOOK_v103.md\n",
      "Cleaned markdown version saved to: Almanack/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_EBOOK_v103_cleaned.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renlephy/miniconda3/envs/python-api/lib/python3.12/site-packages/ebooklib/epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n",
      "/home/renlephy/miniconda3/envs/python-api/lib/python3.12/site-packages/ebooklib/epub.py:1423: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    }
   ],
   "source": [
    "# Import the custom module\n",
    "import os\n",
    "import textPreprocessing as tp\n",
    "\n",
    "input_file = 'assets/Almanack/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_EBOOK_v103.epub'\n",
    "output_markdown = input_file.replace('.epub', '.md')\n",
    "\n",
    "# input_file = 'Olthaus/18_Olthaus_Master.pdf'\n",
    "# output_markdown = input_file.replace('.pdf', '.md')\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Input file {input_file} not found. Please check the path.\")\n",
    "else:\n",
    "    print(f\"Found input file: {input_file}\")\n",
    "\n",
    "converted_path = tp.convert_book_to_markdown(input_file, output_markdown)\n",
    "print(f\"Markdown version saved to: {converted_path}\")\n",
    "\n",
    "# Cleaning markdown text\n",
    "markdown = \"\"\n",
    "with open(output_markdown, 'r', encoding='utf-8') as file:\n",
    "    markdown = file.read()\n",
    "if not markdown:\n",
    "    print(\"Markdown file is empty. Please check the conversion process.\")\n",
    "\n",
    "cleaned_markdown = tp.clean_markdown_text(markdown)\n",
    "\n",
    "# Save the cleaned markdown text to a new file\n",
    "cleaned_markdown_path = converted_path.replace('.md', '_cleaned.md')\n",
    "with open(cleaned_markdown_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_markdown)\n",
    "print(f\"Cleaned markdown version saved to: {cleaned_markdown_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e77748c",
   "metadata": {},
   "source": [
    "# Step 2: Text Segmentation\n",
    "The text is then segmented into smaller parts for easier processing of the embeddings. The segmentation is done using the `textSegmentation` module. Two methods are provided for segmentation: \n",
    "1. **NLTK**: This method uses the Natural Language Toolkit (NLTK) for text segmentation.\n",
    "2. **LangChain**: This method uses the LangChain library for text segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c2ce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NLTK for text segmentation.\n",
      "NLTK TextTiling produced 914 segments.\n"
     ]
    }
   ],
   "source": [
    "# Import the segmentation functions from your module\n",
    "import textSegmentation as ts\n",
    "\n",
    "# Load your text from a file (e.g., your Markdown version of the book)\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "book_text = cleaned_markdown\n",
    "\n",
    "method = 'NLTK'\n",
    "# method = 'LangChain'\n",
    "\n",
    "segments = []\n",
    "\n",
    "if method == 'NLTK':\n",
    "    print(\"Using NLTK for text segmentation.\")\n",
    "\n",
    "    # Option 1: Segment using NLTK's TextTiling\n",
    "    segments = ts.segment_text_texttiling(book_text, w=5, k=2)\n",
    "    print(\"NLTK TextTiling produced\", len(segments), \"segments.\")\n",
    "\n",
    "if method == 'LangChain':\n",
    "    print(\"Using LangChain for text segmentation.\")\n",
    "    \n",
    "    # Option 2: Segment using LangChain's splitter\n",
    "    try:\n",
    "        segments = ts.segment_text_langchain(book_text, chunk_size=512, chunk_overlap=50)\n",
    "        print(\"LangChain splitter produced\", len(segments), \"chunks.\")\n",
    "    except ImportError as e:\n",
    "        print(\"LangChain not installed. Please install it via 'pip install langchain'.\")\n",
    "\n",
    "\n",
    "# Save the segments to a file\n",
    "ts.save_segments_to_file(segments, input_file.replace('.epub', '_segments.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e7d43",
   "metadata": {},
   "source": [
    "# Step 3: Embedding Generation\n",
    "The segmented text is then converted into embeddings using the Sentence Transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1c9fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 09:34:47,547 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-04-10 09:34:47,547 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302cbe69f49d42afb6d425a61972858f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set logging to INFO level so that you can see more details in console output\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Encode with the progress bar enabled\n",
    "embeddings = model.encode(segments, show_progress_bar=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18671a8",
   "metadata": {},
   "source": [
    "# Step 4: Post Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edd91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of text segments to embeddings saved to: Almanack/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_EBOOK_v103_mapped_embeddings.json\n"
     ]
    }
   ],
   "source": [
    "import textPostprocessing as tpost\n",
    "\n",
    "mappedEmbeddingOutput = input_file.replace('.epub', '_mapped_embeddings.json')\n",
    "tpost.map_text_to_embeddings(segments, embeddings, mappedEmbeddingOutput)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224fb1f5",
   "metadata": {},
   "source": [
    "# Step 5: Querying the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7772003c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 09:45:07,470 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-04-10 09:45:07,470 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ec0d9cd10f4e2da9cc7151478122d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment ID: 407\n",
      "Similarity: 0.5959\n",
      "Text snippet: Reading was my first love. [4]\n",
      "\n",
      "I remember my grandparents’ house in India. I’d be a little kid on the floor going through all of my grandfather’s Reader’s Digests, which is all he had to read. Now, of course, there’s a smorgasbord of information out there—anybody can read anything all the time. Back then, it was much more limited. I would read comic books, storybooks, whatever I could get my hands on. ...\n",
      "--------------------------------------------------------------------------------\n",
      "Segment ID: 408\n",
      "Similarity: 0.5905\n",
      "Text snippet: I think I always loved to read because I’m actually an antisocial introvert. I was lost in the world of words and ideas from an early age. I think some of it comes from the happy circumstance that when I was young, nobody forced me to read certain things. ...\n",
      "--------------------------------------------------------------------------------\n",
      "Segment ID: 424\n",
      "Similarity: 0.5731\n",
      "Text snippet: Explain what you learned to someone else. Teaching forces learning.\n",
      "\n",
      "It’s not about “educated” vs. “uneducated.” It’s about “likes to read” and “doesn’t like to read.” ...\n",
      "--------------------------------------------------------------------------------\n",
      "Segment ID: 406\n",
      "Similarity: 0.5658\n",
      "Text snippet: The genuine love for reading itself, when cultivated, is a superpower. We live in the age of Alexandria, when every book and every piece of knowledge ever written down is a fingertip away. The means of learning are abundant—it’s the desire to learn that is scarce. [3] ...\n",
      "--------------------------------------------------------------------------------\n",
      "Segment ID: 414\n",
      "Similarity: 0.5404\n",
      "Text snippet: I don’t actually read a lot of books. I pick up a lot of books and only get through a few which form the foundation of my knowledge. ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import textQuerying as tq\n",
    "\n",
    "# Specify the path to your mapped embedding file.\n",
    "mapping_file = mappedEmbeddingOutput\n",
    "\n",
    "# Define a query string.\n",
    "query = \"loves reading. Educated and uneducated.\"\n",
    "\n",
    "# Load your SentenceTransformer model (should be the same model used in step 3).\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Retrieve the top 5 matching segments.\n",
    "top_matches = tq.query_mapped_embeddings(query, mapping_file, model, top_k=5)\n",
    "\n",
    "# Print out the top matching segments\n",
    "for match in top_matches:\n",
    "    print(f\"Segment ID: {match['id']}\")\n",
    "    print(f\"Similarity: {match['similarity']:.4f}\")\n",
    "    print(\"Text snippet:\", match['text'], \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b759b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
